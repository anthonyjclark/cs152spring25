{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031555e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import Linear, Module\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87598f3d",
   "metadata": {},
   "source": [
    "# Prepare The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0758dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_curve(X: torch.Tensor, noise: float) -> torch.Tensor:\n",
    "    return torch.sin(X) + torch.randn(X.shape) * noise\n",
    "\n",
    "\n",
    "class SinusoidDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, num_features=1, num_targets=1, noise=0.1):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "        self.num_targets = num_targets\n",
    "        self.noise = noise\n",
    "\n",
    "        self.X = torch.rand(num_samples, num_features) * 2 * torch.pi - torch.pi\n",
    "        self.y = demo_curve(self.X, noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset specifications\n",
    "\n",
    "N = 100\n",
    "nx = 1\n",
    "ny = 1\n",
    "\n",
    "noise = 0.1\n",
    "\n",
    "full_dataset = SinusoidDataset(N, nx, ny, noise)\n",
    "\n",
    "# Split it into training and validation partitions\n",
    "\n",
    "data_split = [0.8, 0.2]\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, data_split)\n",
    "\n",
    "plt.plot(full_dataset.X, full_dataset.y, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f9d56",
   "metadata": {},
   "source": [
    "# Design A Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetwork(Module):\n",
    "    def __init__(self, num_features, num_hidden, num_targets, activation):\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.layer1 = Linear(num_features, num_hidden)\n",
    "        self.layer2 = Linear(num_hidden, num_targets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ae1c2",
   "metadata": {},
   "source": [
    "# Train With (Batch) Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n1 = 10\n",
    "activation = F.relu\n",
    "\n",
    "batch_size = len(train_dataset)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "# Create data loaders for the training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = TwoLayerNetwork(nx, n1, ny, activation)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Run batch gradient descent\n",
    "for _ in range(num_epochs):\n",
    "\n",
    "    #\n",
    "    # Put the model in training mode and update the parameters\n",
    "    #\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Grab the entire dataset as a single batch\n",
    "    X, y = next(iter(train_loader))\n",
    "\n",
    "    yhat = model(X)\n",
    "\n",
    "    loss = criterion(yhat, y)\n",
    "    train_losses.append(loss.detach().item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #\n",
    "    # Put the model in evaluation mode and compute the validation loss\n",
    "    #\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X, y = next(iter(valid_loader))\n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        valid_losses.append(loss.detach().item())\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "test_X = torch.linspace(-np.pi, np.pi, 100).unsqueeze(dim=1)\n",
    "test_y = model(test_X).detach().numpy()\n",
    "\n",
    "axes[0].plot(full_dataset.X, full_dataset.y, \"o\", label=\"Data\")\n",
    "axes[0].plot(test_X, test_y, label=\"Model Output\")\n",
    "axes[0].set_title(\"Model Fit\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(train_losses, label=\"Training Loss\")\n",
    "axes[1].plot(valid_losses, label=\"Validation Loss\")\n",
    "axes[1].set_title(\"Epoch VS Loss\")\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ebea4d",
   "metadata": {},
   "source": [
    "# Train With Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f90a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n1 = 10\n",
    "activation = F.relu\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "# Create data loaders for the training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = TwoLayerNetwork(nx, n1, ny, activation)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Run batch gradient descent\n",
    "for _ in range(num_epochs):\n",
    "\n",
    "    #\n",
    "    # Put the model in training mode and update the parameters\n",
    "    #\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Grab the entire dataset as a single batch\n",
    "    for X, y in train_loader:\n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        train_losses.append(loss.detach().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #\n",
    "    # Put the model in evaluation mode and compute the validation loss\n",
    "    #\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X, y = next(iter(valid_loader))\n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        valid_losses.append(loss.detach().item())\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "test_X = torch.linspace(-np.pi, np.pi, 100).unsqueeze(dim=1)\n",
    "test_y = model(test_X).detach().numpy()\n",
    "\n",
    "axes[0].plot(full_dataset.X, full_dataset.y, \"o\", label=\"Data\")\n",
    "axes[0].plot(test_X, test_y, label=\"Model Output\")\n",
    "axes[0].set_title(\"Model Fit\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(\n",
    "    torch.linspace(1, num_epochs, len(train_losses)),\n",
    "    train_losses,\n",
    "    label=\"Training Loss\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    torch.linspace(1, num_epochs, len(valid_losses)),\n",
    "    valid_losses,\n",
    "    label=\"Validation Loss\",\n",
    ")\n",
    "axes[1].set_title(\"Epoch VS Loss\")\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846308d5",
   "metadata": {},
   "source": [
    "# Train With Minibatch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n1 = 10\n",
    "activation = F.relu\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "# Create data loaders for the training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = TwoLayerNetwork(nx, n1, ny, activation)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Run batch gradient descent\n",
    "for _ in range(num_epochs):\n",
    "\n",
    "    #\n",
    "    # Put the model in training mode and update the parameters\n",
    "    #\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Grab the entire dataset as a single batch\n",
    "    for X, y in train_loader:\n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        train_losses.append(loss.detach().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #\n",
    "    # Put the model in evaluation mode and compute the validation loss\n",
    "    #\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X, y = next(iter(valid_loader))\n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        valid_losses.append(loss.detach().item())\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "test_X = torch.linspace(-np.pi, np.pi, 100).unsqueeze(dim=1)\n",
    "test_y = model(test_X).detach().numpy()\n",
    "\n",
    "axes[0].plot(full_dataset.X, full_dataset.y, \"o\", label=\"Data\")\n",
    "axes[0].plot(test_X, test_y, label=\"Model Output\")\n",
    "axes[0].set_title(\"Model Fit\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(\n",
    "    torch.linspace(1, num_epochs, len(train_losses)),\n",
    "    train_losses,\n",
    "    label=\"Training Loss\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    torch.linspace(1, num_epochs, len(valid_losses)),\n",
    "    valid_losses,\n",
    "    label=\"Validation Loss\",\n",
    ")\n",
    "axes[1].set_title(\"Epoch VS Loss\")\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e32500",
   "metadata": {},
   "source": [
    "# Manually Handling Minibatches\n",
    "\n",
    "Here is my original version of minibatch SGD. I am leaving it here so that you can see how one might manually create batches instead of relying on the dataset+dataloader approach.\n",
    "\n",
    "```python\n",
    "\n",
    "# Hyperparameters\n",
    "n1 = 10\n",
    "activation = F.relu\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.2\n",
    "\n",
    "num_batches = x.shape[0] // batch_size\n",
    "\n",
    "model = TwoLayerNetwork(nx, n1, ny, activation)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "for _ in range(num_epochs):\n",
    "    shuffled_indices = torch.randperm(x.shape[0])\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "\n",
    "        xb = x[shuffled_indices[batch*batch_size:batch*batch_size+batch_size]]\n",
    "        yb = y[shuffled_indices[batch*batch_size:batch*batch_size+batch_size]]\n",
    "\n",
    "        yhatb = model(xb)\n",
    "        lossb = criterion(yhatb, yb)\n",
    "        losses.append(lossb.detach().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lossb.backward()\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbedca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "434e3f2d58e1385e0adb0e032cbe799909e99708e62ae45506af3a1338bb2ba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
