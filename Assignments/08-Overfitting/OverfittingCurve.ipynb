{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "633ff60f",
   "metadata": {},
   "source": [
    "# Overfitting a Curve\n",
    "\n",
    "In this assignment, you will play around with the various models and corresponding parameters. \n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "Things to try:\n",
    "\n",
    "- **Before you run any code**, read through the notebook and make some predictions. What do you expect to see for the different models?\n",
    "    + linear\n",
    "    + quadratic\n",
    "    + cubic\n",
    "    + n-degree polynomial\n",
    "    + ordinary least squares\n",
    "    + neural network\n",
    "- Now run the notebook. What surprised you? What matched your expectations?\n",
    "- Now report on your results with the following:\n",
    "    + Changing the number of degrees in the polynomial model.\n",
    "    + Using a non-zero weight decay.\n",
    "    + Changing the number of layers in the neural network model.\n",
    "    + Changing the number of training samples.\n",
    "- Finally, open the `OverfittingFashionMNIST.ipynb` and see if you can get the neural network to overfit the data (get the bad thing to happen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a479",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dd3de",
   "metadata": {},
   "source": [
    "## Create Fake Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73634f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubicDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, input_range: tuple[float, float]):\n",
    "\n",
    "        # Internal function to generate fake data\n",
    "        def fake_y(x, add_noise=False):\n",
    "            y = 10 * x ** 3 - 5 * x\n",
    "            return y + torch.randn_like(y) * 0.5 if add_noise else y\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.input_range = input_range\n",
    "\n",
    "        min_x, max_x = input_range\n",
    "\n",
    "        # True curve for plotting purposes\n",
    "        true_N = 100\n",
    "        self.true_X = torch.linspace(min_x, max_x, true_N).reshape(-1, 1)\n",
    "        self.true_y = fake_y(self.true_X)\n",
    "\n",
    "        self.X = torch.rand(self.num_samples).reshape(-1, 1) * (max_x - min_x) + min_x\n",
    "        self.y = fake_y(self.X, add_noise=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def num_features(self):\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    def num_outputs(self):\n",
    "        return self.y.shape[1]\n",
    "\n",
    "    def plot(self, model=None, losses=None):\n",
    "\n",
    "        # Plot loss curves if given\n",
    "        train_losses = losses[0] if losses else None\n",
    "        valid_losses = losses[1] if losses else None\n",
    "        plot_losses = train_losses != None and len(train_losses) > 1\n",
    "\n",
    "        _, axes = plt.subplots(1, 2, figsize=(16, 8)) if plot_losses else plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax1: plt.Axes = axes[0] if plot_losses else axes\n",
    "        ax2: plt.Axes | None = axes[1] if plot_losses else None\n",
    " \n",
    "        ax1.plot(self.X, self.y, \"o\", label=\"Noisy Samples\")\n",
    "        ax1.plot(self.true_X, self.true_y, label=\"Baseline Curve\")\n",
    "\n",
    "        # Plot the model's learned regression function\n",
    "        if model:\n",
    "            with torch.no_grad():\n",
    "                pred_y = model(self.true_X)\n",
    "\n",
    "            ax1.plot(self.true_X, pred_y.squeeze(), label=\"Learned Model\")\n",
    "\n",
    "        ax1.set_xlim(self.input_range)\n",
    "        ax1.set_ylim(-5, 5)\n",
    "        ax1.legend()\n",
    "        ax1.set_xlabel(\"x\")\n",
    "        ax1.set_ylabel(\"y\")\n",
    "\n",
    "        # Plot training and validation losses\n",
    "        if plot_losses and ax2:\n",
    "            ax2.plot(torch.linspace(1, num_epochs, len(train_losses)), train_losses, label=\"Training\")\n",
    "            ax2.plot(torch.linspace(1, num_epochs, len(valid_losses)), valid_losses, label=\"Validation\")\n",
    "            ax2.legend()\n",
    "            ax2.set_xlabel(\"Epoch\")\n",
    "            ax2.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples/examples\n",
    "N = 25\n",
    "train_valid_split = [0.8, 0.2]\n",
    "batch_size = N // 4\n",
    "\n",
    "# Range of training data input\n",
    "MIN_X, MAX_X = -1, 1\n",
    "\n",
    "cubic_dataset = CubicDataset(N, (MIN_X, MAX_X))\n",
    "cubic_dataset.plot()\n",
    "\n",
    "train_dataset, valid_dataset = random_split(cubic_dataset, train_valid_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "595ba9b6",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, num_epochs, weight_decay, model):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for _ in progress_bar(range(num_epochs)):\n",
    "\n",
    "        # Model can be an nn.Module or a function\n",
    "        # If it is a PyTorch module, then put it into training mode\n",
    "        if isinstance(model, nn.Module):\n",
    "            model.train()\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            yhat = model(X)\n",
    "\n",
    "            loss = criterion(yhat, y)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # If the model is a PyTorch module, then put it into evaluation mode\n",
    "        if isinstance(model, nn.Module):\n",
    "            model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in valid_loader:\n",
    "                yhat = model(X)\n",
    "                loss = criterion(yhat, y)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bb598",
   "metadata": {},
   "source": [
    "## Train a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    \"\"\"A simple linear model.\n",
    "\n",
    "    This is roughly equivalent to the following implementation, which would also work.\n",
    "    \n",
    "    # Model parameters\n",
    "    m = torch.randn(1, requires_grad=True)\n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    \n",
    "    def model(x):\n",
    "        return m * x + b\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_outputs):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create the model (uncomment the second line to print it out)\n",
    "model = Linear(cubic_dataset.num_features(), cubic_dataset.num_outputs())\n",
    "# print(list(model.parameters()))\n",
    "\n",
    "# Train and plot the results\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9b37f",
   "metadata": {},
   "source": [
    "## Train a Quadratic Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd011eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "class Quadratic(nn.Module):\n",
    "    \"\"\"A quadratic model.\n",
    "\n",
    "    This is roughly equivalent to the following implementation (note the x**2 below).\n",
    "    \n",
    "    # Model parameters\n",
    "    w2 = torch.randn(1, requires_grad=True)\n",
    "    w1 = torch.randn(1, requires_grad=True)\n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    \n",
    "    # Create simple quadratic model\n",
    "    def model(x):\n",
    "        return b + w1 * x + w2 * x ** 2\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Let's have a squared feature for every input feature\n",
    "        total_features = num_features * 2\n",
    "        \n",
    "        self.linear = nn.Linear(total_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We're given the linear feature and we must create the squared feature\n",
    "        x_squared = x**2\n",
    "\n",
    "        # Now let's stack x and x**2 together\n",
    "        x_new = torch.hstack([x, x_squared])\n",
    "\n",
    "        return self.linear(x_new)\n",
    "\n",
    "model = Quadratic(cubic_dataset.num_features(), cubic_dataset.num_outputs())\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db198adc",
   "metadata": {},
   "source": [
    "## Train a Cubic Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "class Cubic(nn.Module):\n",
    "    \"\"\"A Cubic model.\n",
    "\n",
    "    This is roughly equivalent to the following implementation.\n",
    "    \n",
    "    # Model parameters\n",
    "    w3 = torch.randn(1, requires_grad=True)\n",
    "    w2 = torch.randn(1, requires_grad=True)\n",
    "    w1 = torch.randn(1, requires_grad=True)\n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    \n",
    "    # Create simple cubic model\n",
    "    def model(x):\n",
    "        return b + w1 * x + w2 * x ** 2 + w3 * x ** 3\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Let's have squared and cubic features for every input feature\n",
    "        total_features = num_features * 3\n",
    "        \n",
    "        self.linear = nn.Linear(total_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_squared = x**2\n",
    "        x_cubed = x**3\n",
    "        x_new = torch.hstack([x, x_squared, x_cubed])\n",
    "        return self.linear(x_new)\n",
    "\n",
    "\n",
    "model = Cubic(cubic_dataset.num_features(), cubic_dataset.num_outputs())\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44575ec",
   "metadata": {},
   "source": [
    "## Train a Polynomial Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "class Polynomial(nn.Module):\n",
    "    \"\"\"A polynomial model.\n",
    "\n",
    "    This is roughly equivalent to the following implementation.\n",
    "    \n",
    "    # Model parameters\n",
    "    powers = torch.arange(degrees + 1)\n",
    "    params = torch.randn(degrees + 1, requires_grad=True)\n",
    "    \n",
    "    # Create simple cubic model\n",
    "    def model(x):\n",
    "        X_polynomials = x.pow(powers)\n",
    "        return X_polynomials @ params\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_outputs, highest_degree):\n",
    "        super().__init__()\n",
    "\n",
    "        # We'll have a feature for each degree (including degree-0 --> the bias/intercept)\n",
    "        total_features = highest_degree + 1\n",
    "\n",
    "        # A tensor of the polynomial exponents (\"powers\")\n",
    "        self.powers = torch.arange(total_features)\n",
    "\n",
    "        self.linear = nn.Linear(total_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Try printing `x_polynomials` and looking at its shape\n",
    "        x_polynomials = x.pow(self.powers)\n",
    "        return self.linear(x_polynomials)\n",
    "\n",
    "\n",
    "# Try different degree polynomial models\n",
    "highest_degree = 50  # 3, 4, 16, 32, 64, 128\n",
    "model = Polynomial(cubic_dataset.num_features(), cubic_dataset.num_outputs(), highest_degree)\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663def1",
   "metadata": {},
   "source": [
    "## Compute Polynomial Model Using Ordinary Least Squares\n",
    "\n",
    "See [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3a288-5d68-4b72-b994-4842ce88c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_features_to_polynomial(x, highest_degree):\n",
    "    powers = torch.arange(highest_degree + 1)\n",
    "    return x.unsqueeze(-1).pow(powers), powers\n",
    "\n",
    "# Reshaping the training dataset for use with ordinary least squares\n",
    "# Hint: try printing the shape of these tensors\n",
    "train_X = torch.tensor([x for x, _ in train_dataset])\n",
    "train_y = torch.tensor([y for _, y in train_dataset])\n",
    "\n",
    "highest_degree = 10  # 3, 4, 16, 32, 64, 128\n",
    "train_X_polynomial, powers = input_features_to_polynomial(train_X, highest_degree)\n",
    "\n",
    "# Compute \"optimal\" parameters using OLS (not gradient descent)\n",
    "ols_derived_parameters = ((train_X_polynomial.T @ train_X_polynomial).inverse() @ train_X_polynomial.T) @ train_y\n",
    "\n",
    "def model(x):\n",
    "    x_polynomials, _ = input_features_to_polynomial(x, highest_degree)\n",
    "    return x_polynomials @ ols_derived_parameters\n",
    "\n",
    "\n",
    "# Compute loss and plot the results\n",
    "mse = nn.functional.mse_loss(model(train_X), train_y)\n",
    "cubic_dataset.plot(model, losses=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3feed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the average parameter magnitude (compare this to the network below)\n",
    "ols_derived_parameters.abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d5fbc",
   "metadata": {},
   "source": [
    "## Train Neural Network Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # For regression we should use a linear output layer\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "weight_decay = 0\n",
    "\n",
    "layer_sizes = (1, 100, 100, 100, 1)\n",
    "\n",
    "model = NeuralNetwork(layer_sizes)\n",
    "summary(model)\n",
    "\n",
    "# x = train_X.unsqueeze(-1)\n",
    "\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9927f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af2d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
