{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f14027b",
   "metadata": {},
   "source": [
    "# Activations, Initialization, and Normalization\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Clone this repository (or just pull changes if you already have it).\n",
    "2. Start Jupyter (don't forget to select the correct kernel/environment).\n",
    "3. Duplicate this file so that you can still pull changes without merging.\n",
    "4. Complete the \"Questions to Answer.\"\n",
    "\n",
    "## Notes\n",
    "\n",
    "This notebook is all about the range of numbers. Specifically, the range of\n",
    "\n",
    "- input features (pixels, words, distances, stock data, audio waves, etc.),\n",
    "- activation function inputs (weighted sum of outputs from previous layer)\n",
    "- activation function outputs (sigmoid, relu, etc.),\n",
    "- parameters (weights, biases, etc.), and\n",
    "- parameter gradients.\n",
    "\n",
    "Here are a couple of reminders that you might find helpful.\n",
    "\n",
    "Each neuron implements these two equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]} &= A^{[l-1]} W^{[l]T} + \\mathbf{b}^{[l]}\\\\\n",
    "A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $Z^{[l]}$ is the linear output of layer $l$ (e.g., the output of a `nn.Linear` module)\n",
    "- $A^{[l-1]}$ is the activation output for layer $l-1$\n",
    "- $W^{[l]}$ is a parameter matrix for layer $l$ called \"weights\"\n",
    "- $\\mathbf{b}^{[l]}$ is a parameter vector for layer $l$ called \"bias\"\n",
    "- $A^{[l]}$ is the activations for layer $l$ (outputs of an activation function, e.g., `nn.Sigmoid`)\n",
    "- $g^{[l]}(\\cdot)$ is the activation function for layer $l$ (e.g., sigmoid)\n",
    "\n",
    "Here are a couple of activation function examples. Pay close attention to the range of the input values (input values are on the x-axis).\n",
    "\n",
    "![Sigmoid Activation Function](https://singlepages.github.io/NeuralNetworks/img/Sigmoid.png)\n",
    "\n",
    "![ReLU Activation Function](https://singlepages.github.io/NeuralNetworks/img/ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643335ef",
   "metadata": {},
   "source": [
    "## Questions to Answer\n",
    "\n",
    "These questions will also appear on gradescope.\n",
    "\n",
    "1. What terms directly impact the output of an activation function?\n",
    "1. What is the purpose of the bias term in a single neuron?\n",
    "1. What is the purpose of an activation function (what happens when we remove all activation functions)?\n",
    "1. Where is the \"interesting\" / \"useful\" range for most activation functions?\n",
    "1. Why are deeper networks generally more useful than shallower networks?\n",
    "1. What happens to gradients in deeper networks?\n",
    "1. Why is it an issue for input features to be in the range from 25 to 35?\n",
    "1. What is a \"good\" range for input features?\n",
    "1. Can we \"normalize\" values between layers?\n",
    "1. What is the goal when initializing network parameters?\n",
    "\n",
    "You can use the code below to help find (or confirm) answers to these conceptual questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195386c1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76aaf9",
   "metadata": {},
   "source": [
    "## Experimental Parameters and Hyperparameters\n",
    "\n",
    "I recommend the following process for running quick experiments:\n",
    "\n",
    "1. Change a value in the cell below.\n",
    "2. Run the entire notebook (or at least from here down).\n",
    "3. Examine the output plots.\n",
    "\n",
    "You can also edit and add any code that you find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitMethod(Enum):\n",
    "    \"\"\"A list of initialization methods for the weights of a neural network.\"\"\"\n",
    "    Zeros = 0\n",
    "    Ones = 1\n",
    "    Large = 2\n",
    "    Uniform = 3\n",
    "    Normal = 4\n",
    "    Normal2 = 5\n",
    "    Xavier = 6\n",
    "    Kaiming = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples/examples\n",
    "N = 1000\n",
    "train_valid_split = [0.8, 0.2]\n",
    "batch_size = 32\n",
    "\n",
    "# Range of training data input (try something shifted away from 0)\n",
    "input_range = (-3, 3)\n",
    "\n",
    "# Noise factor for training data input (what happens if noise is too high?)\n",
    "input_noise = 0.1\n",
    "\n",
    "# Horizontal shift of the training data input (useful for demonstrating input normalization)\n",
    "input_shift = 1\n",
    "\n",
    "# Noise factor for training data output (what happens if noise is too high?)\n",
    "output_noise = 0.1\n",
    "\n",
    "# Vertically shift the sinusoidal output (useful for demonstrating use of bias)\n",
    "output_shift = 0.5\n",
    "\n",
    "# Neural network activation function\n",
    "#   Options: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "#   You should also try nn.Identity\n",
    "activation_module = nn.ReLU\n",
    "\n",
    "# Neural network output neuron bias (should the output layer include bias?)\n",
    "output_bias = True\n",
    "\n",
    "# Neural network architecture (useful for comparing width and depth)\n",
    "#   neuron : layer_sizes = []                      # No hidden layers\n",
    "#   wider  : layer_sizes = [100]                   # One hidden layer with 100 neurons\n",
    "#   deep   : layer_sizes = [8] * 3                 # Three hidden layers with 8 neurons each\n",
    "#   deeper : layer_sizes = [80, 90, 80, 70, 80, 5] # Six hidden layers with 80, 90, 80, 70, 80, 5 neurons\n",
    "neurons_per_hidden_layer = [8, 8]\n",
    "\n",
    "# Neural network parameter initialization method (see the enumeration above)\n",
    "initialization_method = InitMethod.Kaiming\n",
    "\n",
    "# Number of training epochs (useful for examining problematic gradients, set to 1)\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7614d",
   "metadata": {},
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int,\n",
    "        input_range: tuple[float, float],\n",
    "        input_shift: float,\n",
    "        input_noise: float,\n",
    "        output_shift: float,\n",
    "        output_noise: float,\n",
    "    ):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # Sinusoidal data without noise\n",
    "        self.X_no_noise = torch.linspace(*input_range, num_samples).reshape(-1, 1) + input_shift\n",
    "        self.y_no_noise = torch.sin(self.X_no_noise) + output_shift\n",
    "\n",
    "        # Sinusoidal data with noise\n",
    "        self.X = self.X_no_noise + torch.randn(self.X_no_noise.shape) * input_noise\n",
    "        self.y = torch.sin(self.X) + torch.randn(self.X.shape) * output_noise + output_shift\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(self.X, self.y, \"o\", label=\"Noisy Training Data\")\n",
    "        plt.plot(self.X_no_noise, self.y_no_noise, label=\"Baseline Sinusoid\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0394377",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinusoid_dataset = SinusoidDataset(N, input_range, input_shift, input_noise, output_shift, output_noise)\n",
    "sinusoid_dataset.plot()\n",
    "train_dataset, valid_dataset = random_split(sinusoid_dataset, train_valid_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86b3e",
   "metadata": {},
   "source": [
    "## Fully-Connected Neural Network With Linear Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02170aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, neurons_per_hidden_Layer: list[int], output_bias: bool, act_mod: nn.Module):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Internal function for creating a linear layer with an activation\n",
    "        def layer(nlm1: int, nl: int) -> nn.Module:\n",
    "            linear = nn.Linear(nlm1, nl)\n",
    "            # Optionally group the linear layer with an activation function\n",
    "            return nn.Sequential(linear, act_mod()) if act_mod else linear\n",
    "\n",
    "        # Add the input and output layers\n",
    "        neurons_per_layer = [1] + neurons_per_hidden_Layer + [1]\n",
    "\n",
    "        # Hidden layers\n",
    "        hidden_layers = []\n",
    "        if len(neurons_per_hidden_Layer) > 0:\n",
    "            hidden_layers = [\n",
    "                layer(nlminus1, nl)\n",
    "                for nlminus1, nl in zip(neurons_per_layer[:-2], neurons_per_layer[1:-1])\n",
    "            ]\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = nn.Linear(neurons_per_layer[-2], neurons_per_layer[-1], bias=output_bias)\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return self.layers(X)\n",
    "\n",
    "\n",
    "def initialize_parameters(layer: nn.Module):\n",
    "    # Ignore any non-parameter layers (e.g., activation functions)\n",
    "    if type(layer) == nn.Linear:\n",
    "        print(\"Initializing\", layer)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # We can always initialize bias to zero\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.fill_(0.0)\n",
    "\n",
    "            # Initialize the weights using the specified method\n",
    "            match initialization_method:\n",
    "                case InitMethod.Zeros:\n",
    "                    layer.weight.fill_(0.0)\n",
    "\n",
    "                case InitMethod.Ones:\n",
    "                    layer.weight.fill_(1.0)\n",
    "\n",
    "                case InitMethod.Large:\n",
    "                    layer.weight.set_(torch.rand_like(layer.weight) * 10.0)\n",
    "\n",
    "                case InitMethod.Uniform:\n",
    "                    layer.weight.uniform_()\n",
    "\n",
    "                case InitMethod.Normal:\n",
    "                    layer.weight.normal_()\n",
    "\n",
    "                case InitMethod.Normal2:\n",
    "                    fan_out = torch.sqrt(torch.tensor(layer.weight.shape[0]))\n",
    "                    layer.weight.normal_() * (1 / fan_out)\n",
    "\n",
    "                case InitMethod.Xavier:\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "                case InitMethod.Kaiming:\n",
    "                    nn.init.kaiming_normal_(layer.weight)\n",
    "                \n",
    "                case _:\n",
    "                    print(f\"'{initialization_method}' is not a valid initialization method\")\n",
    "\n",
    "\n",
    "def report_mean_stdev(tensor: torch.Tensor, label: str, indent=\"  \"):\n",
    "    std, mean = torch.std_mean(tensor) if tensor.numel() > 1 else (torch.tensor([0]), torch.mean(tensor))\n",
    "    print(f\"{indent}{label} Mean  = {mean.item():.3f}\")\n",
    "    print(f\"{indent}{label} Stdev = {std.item():.3f}\\n\")\n",
    "\n",
    "\n",
    "def report_layer_info(l: int, layer: nn.Module, A=None):\n",
    "    print(f\"---------------- Layer {l} ---------------\")\n",
    "    print(layer, \"\\n\")\n",
    "    if A is not None:\n",
    "        report_mean_stdev(A, \"Layer Input\")\n",
    "\n",
    "    if type(layer) == nn.Sequential or type(layer) == nn.Linear:\n",
    "        W = layer.weight if type(layer) == nn.Linear else layer[0].weight\n",
    "        b = layer.bias if type(layer) == nn.Linear else layer[0].bias\n",
    "        report_mean_stdev(W, \"Weights\")\n",
    "        if b is not None:\n",
    "            report_mean_stdev(b, \"Bias\")\n",
    "        if W.grad is not None:\n",
    "            report_mean_stdev(W.grad.abs(), \"Weights gradient\")\n",
    "        if b is not None and b.grad is not None:\n",
    "            report_mean_stdev(b.grad.abs(), \"Bias gradient\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee01eb2",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcedc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = NeuralNetwork(neurons_per_hidden_layer, output_bias, activation_module)\n",
    "summary(model)\n",
    "\n",
    "# Use a custom initializer for layer parameters\n",
    "print()\n",
    "model.apply(initialize_parameters)\n",
    "\n",
    "# Report on initial parameter values\n",
    "print()\n",
    "for l, layer in enumerate(model.layers):\n",
    "    report_layer_info(l + 1, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2d137",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and report on the final loss value\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create the training and validation data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in progress_bar(range(num_epochs)):\n",
    "\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        yhat = model(X)\n",
    "\n",
    "        loss = criterion(yhat, y)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader:\n",
    "            yhat = model(X)\n",
    "            loss = criterion(yhat, y)\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "print(f\"Final loss: {train_losses[-1]:.6f}\")\n",
    "\n",
    "# Plot training results\n",
    "_, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "# Plot model predictions\n",
    "yhat = model(sinusoid_dataset.X)\n",
    "ax1.plot(sinusoid_dataset.X, sinusoid_dataset.y, \"o\", label=\"Target\")\n",
    "ax1.plot(sinusoid_dataset.X, yhat.detach(), \"o\", label=\"Prediction\")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot training and validation losses\n",
    "ax2.plot(torch.linspace(1, num_epochs, len(train_losses)), train_losses, label=\"Training\")\n",
    "ax2.plot(torch.linspace(1, num_epochs, len(valid_losses)), valid_losses, label=\"Validation\")\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df5b149",
   "metadata": {},
   "source": [
    "## Examine Hidden Calculations\n",
    "\n",
    "Let's look at the outputs of the neurons that feed into the output neuron (this will just be in the inputs if you create a single neuron model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"hook\" so that we can save the hidden values\n",
    "def capture_layer_input(module: nn.Module, layer_in, layer_out) -> None:\n",
    "    global final_layer_input\n",
    "    final_layer_input = layer_in[0].detach()\n",
    "\n",
    "\n",
    "# Register hook to capture input to final layer\n",
    "final_layer_input = None\n",
    "final_layer = model.layers[-1]\n",
    "final_layer.register_forward_hook(capture_layer_input)\n",
    "\n",
    "# Grab parameters for the final layer\n",
    "WL = final_layer.weight.detach()\n",
    "bL = final_layer.bias.item() if final_layer.bias is not None else 0.0\n",
    "\n",
    "# Plot the baseline sinusoid\n",
    "plt.plot(sinusoid_dataset.X_no_noise, sinusoid_dataset.y_no_noise, \"--\", label=\"Baseline Sinusoid\")\n",
    "\n",
    "# Plot the output of the final layer\n",
    "yhat = model(sinusoid_dataset.X_no_noise)  # Activate hook\n",
    "plt.plot(sinusoid_dataset.X_no_noise, yhat.detach(), \"o\", label=\"Prediction\")\n",
    "\n",
    "# Compare with hand-computed final layer output\n",
    "manually_computed_output = final_layer_input @ WL.T + bL\n",
    "plt.plot(sinusoid_dataset.X_no_noise, manually_computed_output, \"o\", markersize=5, label=\"Combined Activations\")\n",
    "\n",
    "# Plot each input to the final layer\n",
    "individual_activations = final_layer_input * WL\n",
    "plt.plot(sinusoid_dataset.X_no_noise, individual_activations, label=\"Individual Activations\")\n",
    "\n",
    "_ = plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5717b",
   "metadata": {},
   "source": [
    "## Examine Layer Inputs, Parameters, and Gradients\n",
    "\n",
    "We can get a good idea for inter-layer activations and gradients by printing them off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d42886",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Let's call the input to each layer (including the first layer) \"A\"\n",
    "    A = X\n",
    "\n",
    "    # Print the mean and standard deviation for the input (aim for mean 0 and stdev 1)\n",
    "    report_layer_info(0, \"Input Features\", A)\n",
    "\n",
    "    # Do the same for each layer\n",
    "    for l, layer in enumerate(model.layers):\n",
    "        A = layer(A)\n",
    "        report_layer_info(l + 1, layer, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe129a-ce4e-4296-bff3-851eb6f72434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
