{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a1e08ac",
   "metadata": {},
   "source": [
    "# Optimization Assignment\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Clone this repository (or just pull changes if you already have it).\n",
    "2. Start Jupyter (don't forget to select the correct kernel/environment).\n",
    "3. Duplicate this file so that you can still pull changes without merging.\n",
    "4. Complete the \"Things to Implement.\"\n",
    "\n",
    "## Things to Implement\n",
    "\n",
    "1. Momentum\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_{t+1} &:= β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t) \\\\\n",
    "θ_{t+1} &:= θ_t - η m_{t+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2. RMSProp\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g_{t+1}^2 &:= β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2 \\\\\n",
    "θ_{t+1} &:= θ_t - η \\frac{\\nabla_θ L_b(θ_t)}{\\sqrt{g_{t+1}^2} + ε}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "3. Adam\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_{t+1} &:= β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t) \\\\\n",
    "\\hat m_{t+1} &:= \\frac{m_{t+1}}{1 - β_m^t} \\\\\n",
    "g_{t+1}^2 &:= β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2 \\\\\n",
    "\\hat g_{t+1}^2 &:= \\frac{g_{t+1}^2}{1 - β_g^t} \\\\\n",
    "θ_{t+1} &:= θ_t - η \\frac{\\hat m_{t+1}}{\\sqrt{\\hat g_{t+1}^2} + ε}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "A few hints:\n",
    "\n",
    "- Run the code all the way through without any changes and answer the first question on gradescope\n",
    "- Adam combines momentum and RMSProp (and in this case adds a bias correction)\n",
    "- $t$ increments after each update (the actual value of $t$ is only used in Adam)\n",
    "- You'll need to add code in two places (look for @assignment)\n",
    "    1. At the top of the training cell (to initialize momentums and squared gradients)\n",
    "    2. In the parameter update context manager (where you'll find `param -= ...`)\n",
    "- The documentation for [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) will give good values for $\\beta_m$ and $\\beta_g$\n",
    "- If momentum performs poorly, then it might be that you are not updating momentum values **in-place**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c955e",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"~/data\"\n",
    "\n",
    "# Use the GPUs if they are available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Model hyperparameters\n",
    "neurons_per_hidden_layer = [13, 17]\n",
    "\n",
    "# Mini-Batch SGD hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4ad36",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmnist_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "\n",
    "    # Data specific transforms (@normalization: I computed data_mean and data_std as shown in the next cell)\n",
    "    data_mean = (0.2860,)\n",
    "    data_std = (0.3530,)\n",
    "    xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n",
    "\n",
    "    # Training data loader\n",
    "    train_dataset = FashionMNIST(root=path, train=True, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    # Validation data loader\n",
    "    valid_dataset = FashionMNIST(root=path, train=False, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d78186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing normalization constants for Fashion-MNIST\n",
    "# (Commented out since we only need to do this once; and I've done it for you. See @normalization.)\n",
    "# train_loader, valid_loader = get_fmnist_data_loaders(data_path, 0)\n",
    "# X, _ = next(iter(train_loader))\n",
    "# s, m = torch.std_mean(X)\n",
    "\n",
    "train_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n",
    "\n",
    "print(\"Training dataset shape   :\", train_loader.dataset.data.shape)\n",
    "print(\"Validation dataset shape :\", valid_loader.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a58188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a few images as an example\n",
    "num_to_show = 8\n",
    "images = train_loader.dataset.data[:num_to_show]\n",
    "targets = train_loader.dataset.targets[:num_to_show]\n",
    "labels = [train_loader.dataset.classes[t] for t in targets]\n",
    "\n",
    "fig, axes = plt.subplots(1, num_to_show)\n",
    "\n",
    "for axis, image, label in zip(axes, images, labels):\n",
    "    axis.imshow(image.squeeze(), cmap=\"Greys\")\n",
    "    axis.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    axis.set_title(f\"{label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d50c0",
   "metadata": {},
   "source": [
    "## Create a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n",
    "        # NOTE: this layer is not trained\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z; with parameters W and b) and\n",
    "        # 2. a non-linear comonent (computing A; called an activation function)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer] + hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input layer size depends on the dataset\n",
    "nx = train_loader.dataset.data.shape[1:].numel()\n",
    "\n",
    "# The output layer size depends on the dataset\n",
    "ny = len(train_loader.dataset.classes)\n",
    "\n",
    "# Preprend the input and append the output layer sizes\n",
    "layer_sizes = [nx] + neurons_per_hidden_layer + [ny]\n",
    "\n",
    "# Create the model and set it to use the GPU if available\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "summary(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c4f7f",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying model creation here so that the model is recreated each time the cell is run\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "t = 0\n",
    "\n",
    "# \n",
    "# TODO: Add your initialization code for momentum, RMSProp, and Adam\n",
    "# @assignment\n",
    "#\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# A master bar for fancy output progress\n",
    "mb = master_bar(range(num_epochs))\n",
    "\n",
    "# Information for plots\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "    model.train()\n",
    "\n",
    "    train_N = len(train_loader.dataset)\n",
    "    num_train_batches = len(train_loader)\n",
    "    train_dataiterator = iter(train_loader)\n",
    "\n",
    "    train_loss_mean = 0\n",
    "\n",
    "    for batch in progress_bar(range(num_train_batches), parent=mb):\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        train_X, train_Y = next(train_dataiterator)\n",
    "        train_X, train_Y = train_X.to(device), train_Y.to(device)\n",
    "\n",
    "        # Compute the output\n",
    "        train_output = model(train_X)\n",
    "\n",
    "        # Compute loss\n",
    "        train_loss = criterion(train_output, train_Y)\n",
    "\n",
    "        num_in_batch = len(train_X)\n",
    "        tloss = train_loss.item() * num_in_batch / train_N\n",
    "        train_loss_mean += tloss\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Compute partial derivatives\n",
    "        model.zero_grad()\n",
    "        train_loss.backward()\n",
    "        \n",
    "        t += 1\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Original gradient descent\n",
    "            # NOTE: comment this out when  running momentum, RMSProp, or Adam\n",
    "            for param in model.parameters():\n",
    "                # θ_{t+1} := θ_t - η \\nabla_θ L_b(θ_t)\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "            # \n",
    "            # @assignment\n",
    "            # TODO: Add changes for momentum, RMSProp, and Adam\n",
    "            # NOTE: only one should run at a time\n",
    "            #\n",
    "\n",
    "            # @assignment-momentum\n",
    "            # Gradient descent with momentum\n",
    "            # for param, momentum in ...\n",
    "            #     m_{t+1} := β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t)\n",
    "            #     θ_{t+1} := θ_t - η m_{t+1}\n",
    "\n",
    "            # @assignment-rmsprop\n",
    "            # Gradient descent with RMSProp\n",
    "            # for param, sq_grad in ...\n",
    "            #     g_{t+1}^2 := β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2\n",
    "            #     θ_{t+1} := θ_t - η \\frac{\\nabla_θ L_b(θ_t)}{\\sqrt{g_{t+1}^2} + ε}\n",
    "\n",
    "            # @assignment-adam\n",
    "            # Gradient descent with Adam\n",
    "            # for param, momentum, sq_grad in ...\n",
    "            #     m_{t+1} := β_m m_t + (1 - β_m) \\nabla_θ L_b(θ_t)\n",
    "            #     \\hat m_{t+1} := \\frac{m_{t+1}}{1 - β_m^t}\n",
    "            #     g_{t+1}^2 := β_g g_t^2 + (1 - β_g) (\\nabla_θ L_b(θ_t))^2\n",
    "            #     \\hat g_{t+1}^2 := \\frac{g_{t+1}^2}{1 - β_g^t}\n",
    "            #     θ_{t+1} := θ_t - η \\frac{\\hat m_{t+1}}{\\sqrt{\\hat g_{t+1}^2} + ε}\n",
    "\n",
    "    #\n",
    "    # Validation\n",
    "    #\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    valid_N = len(valid_loader.dataset)\n",
    "    num_valid_batches = len(valid_loader)\n",
    "\n",
    "    valid_loss_mean = 0\n",
    "    valid_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # valid_loader is probably just one large batch, so not using progress bar\n",
    "        for valid_X, valid_Y in valid_loader:\n",
    "\n",
    "            valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n",
    "\n",
    "            valid_output = model(valid_X)\n",
    "\n",
    "            valid_loss = criterion(valid_output, valid_Y)\n",
    "\n",
    "            num_in_batch = len(valid_X)\n",
    "            vloss = valid_loss.item() * num_in_batch / valid_N\n",
    "            valid_loss_mean += vloss\n",
    "            valid_losses.append(valid_loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = valid_output.argmax(1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * (valid_correct / valid_N)\n",
    "\n",
    "    # Report information\n",
    "    tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n",
    "    vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n",
    "    vaccu = f\"Valid Accuracy = {(valid_accuracy):>0.1f}%\"\n",
    "    mb.write(f\"[{epoch+1:>2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    valid_Xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[valid_Xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d52b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "434e3f2d58e1385e0adb0e032cbe799909e99708e62ae45506af3a1338bb2ba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
