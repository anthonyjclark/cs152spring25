{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4ceeb0",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Before you start**: open [gradescope](https://www.gradescope.com/) and respond with your initial predictions.\n",
    "\n",
    "In this notebook we'll be using a new (to us) dataset, and you'll compare the performance of several models. Your end goal is to acheive the highest accuracy you have the patience to attain. You will\n",
    "\n",
    "1. Train a ten-neuron network without any hidden layers or activations. The notebook is already setup to do so, you need only run all cells.\n",
    "\n",
    "1. Train a fully connected neural network with multiple hidden layers. You'll only need to change the `neurons_per_hidden_layer` variable.\n",
    "\n",
    "1. Create and train a convolutional neural network. A good place to start is [this tutorial form PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network). The most difficult part is getting the output of the convolutional layers to match the input of the fully connected (linear) layers. Your best tools include: (1) error messages talking about shapes, (2) the call to `summary(model)`, and (3) hand computing the shape of each layer's output.\n",
    "\n",
    "1. Try including [dropout](https://pytorch.org/docs/stable/nn.html#dropout-layers) and/or [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d) layers.\n",
    "\n",
    "1. Tune the Adam optimizer (pass in some parameters).\n",
    "\n",
    "1. Try one of the models [provided by torchvision](https://pytorch.org/vision/stable/models.html). For example:\n",
    "\n",
    "~~~python\n",
    "from torchvision.models import resnet18\n",
    "model = resnet18(num_classes=num_classes).to(device)\n",
    "~~~\n",
    "\n",
    "This notebook contains several \"TODO\" comments. Search these out when trying to tune hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac48da",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f682fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from jupyterthemes import jtplot\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126b053",
   "metadata": {},
   "source": [
    "## Dataset Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "\n",
    "    # Data specific transforms\n",
    "    data_std = (0.2470, 0.2435, 0.2616)\n",
    "    data_mean = (0.4914, 0.4822, 0.4465)\n",
    "    xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n",
    "\n",
    "    # Training dataset and loader\n",
    "    train_dataset = CIFAR10(root=path, train=True, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    valid_dataset = CIFAR10(root=path, train=False, download=True, transform=xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd7c5a",
   "metadata": {},
   "source": [
    "## Training Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(mb, loader, device, model, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    dataiterator = iter(loader)\n",
    "\n",
    "    for batch in progress_bar(range(num_batches), parent=mb):\n",
    "\n",
    "        mb.child.comment = \"Training\"\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        X, Y = next(dataiterator)\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "        # Compute the output\n",
    "        output = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, Y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73463196",
   "metadata": {},
   "source": [
    "## Validation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103de938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(mb, loader, device, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    num_correct = 0\n",
    "\n",
    "    num_classes = len(loader.dataset.classes)\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "\n",
    "    N = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    dataiterator = iter(loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batches = range(num_batches)\n",
    "        batches = progress_bar(batches, parent=mb) if mb else batches\n",
    "        for batch in batches:\n",
    "\n",
    "            if mb:\n",
    "                mb.child.comment = f\"Validation\"\n",
    "\n",
    "            # Grab the batch of data and send it to the correct device\n",
    "            X, Y = next(dataiterator)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            loss = criterion(output, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = output.argmax(dim=1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            comparisons = predictions == Y\n",
    "            num_correct += comparisons.type(torch.float).sum().item()\n",
    "\n",
    "            # Sum up number of correct per class\n",
    "            for result, clss in zip(comparisons, Y):\n",
    "                class_correct[clss] += result.item()\n",
    "                class_total[clss] += 1\n",
    "\n",
    "    accuracy = 100 * (num_correct / N)\n",
    "    accuracies = {\n",
    "        clss: 100 * class_correct[clss] / class_total[clss]\n",
    "        for clss in range(num_classes)\n",
    "    }\n",
    "\n",
    "    return losses, accuracy, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11651d",
   "metadata": {},
   "source": [
    "## Loss Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots(mb, train_losses, valid_losses, epoch, num_epochs):\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    train_xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(0, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[train_xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041f60d",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tune the training batch size\n",
    "train_batch_size = 128\n",
    "\n",
    "# Let's use some shared space for the data (so that we don't have copies\n",
    "# sitting around everywhere)\n",
    "data_path = \"~/data\"\n",
    "\n",
    "# Use the GPUs if they are available\n",
    "# TODO: if you run into GPU memory errors you should set device to \"cpu\" and restart the notebook\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "valid_batch_size = 5000\n",
    "train_loader, valid_loader = get_cifar10_data_loaders(\n",
    "    data_path, train_batch_size, valid_batch_size\n",
    ")\n",
    "\n",
    "# Input and output sizes depend on data\n",
    "num_features = torch.Size(train_loader.dataset.data.shape[1:]).numel()\n",
    "class_names = train_loader.dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ca31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a bunch of images and change the range to [0, 1]\n",
    "nprint = 64\n",
    "images = torch.tensor(train_loader.dataset.data[:nprint] / 255)\n",
    "targets = train_loader.dataset.targets[:nprint]\n",
    "labels = [f\"{class_names[target]:>10}\" for target in targets]\n",
    "\n",
    "# Create a grid of the images (make_grid expects (BxCxHxW))\n",
    "image_grid = make_grid(images.permute(0, 3, 1, 2))\n",
    "\n",
    "_, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.imshow(image_grid.permute(1, 2, 0))\n",
    "ax.grid(None)\n",
    "\n",
    "images_per_row = int(nprint ** 0.5)\n",
    "for row in range(images_per_row):\n",
    "    start_index = row * images_per_row\n",
    "    print(\" \".join(labels[start_index : start_index + images_per_row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d526f6",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57cc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges an image into a column vector\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        # TODO: add dropout and/or batch normalization\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer, *hidden_layers, output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1616b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # TODO: create layers here\n",
    "\n",
    "        # Early CNNs had the following structure:\n",
    "        #    X -> [[Conv2d -> ReLU] x N -> MaxPool2d] x M\n",
    "        #      -> [Linear -> ReLU] x K -> Linear\n",
    "        #   Where\n",
    "        #     0 ≤ N ≤ 3\n",
    "        #     0 ≤ M ≤ 3\n",
    "        #     0 ≤ K < 3\n",
    "        #\n",
    "        # The \"[[Conv2d -> ReLU] x N -> MaxPool2d] x M\" part extracts\n",
    "        # useful features, and the \"[Linear -> ReLU] x K -> Linear\" part\n",
    "        # performs the classification.\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # TODO: implement forward pass here\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try out different network widths and depths\n",
    "neurons_per_hidden_layer = []\n",
    "layer_sizes = [num_features, *neurons_per_hidden_layer, num_classes]\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "# TODO: complete the CNN class in the cell above this one and then uncomment this line\n",
    "# model = CNN().to(device)\n",
    "\n",
    "# TODO: use an off-the-shell model from PyTorch\n",
    "# from torchvision.models import ...\n",
    "# model = ...\n",
    "\n",
    "summary(model)\n",
    "\n",
    "# TODO: try out different Adam hyperparameters\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd179cb",
   "metadata": {},
   "source": [
    "## Training and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tune the number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "accuracies = []\n",
    "\n",
    "# A master bar for fancy output progress\n",
    "mb = master_bar(range(num_epochs))\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "mb.main_bar.comment = f\"Epochs\"\n",
    "\n",
    "# Loss and accuracy prior to training\n",
    "vl, accuracy, _ = validate(None, valid_loader, device, model, criterion)\n",
    "valid_losses.extend(vl)\n",
    "accuracies.append(accuracy)\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    tl = train_one_epoch(mb, train_loader, device, model, criterion, optimizer)\n",
    "    train_losses.extend(tl)\n",
    "\n",
    "    vl, accuracy, acc_by_class = validate(mb, valid_loader, device, model, criterion)\n",
    "    valid_losses.extend(vl)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    update_plots(mb, train_losses, valid_losses, epoch, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, '--o')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks(range(num_epochs+1))\n",
    "plt.ylim([0, 100])\n",
    "\n",
    "max_name_len = max(len(name) for name in class_names)\n",
    "\n",
    "print(\"Accuracy per class\")\n",
    "for clss in acc_by_class:\n",
    "    class_name = class_names[clss]\n",
    "    class_accuracy = acc_by_class[clss]\n",
    "    print(f\"  {class_name:>{max_name_len+2}}: {class_accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3681fc-9d47-45f6-b8d2-c9433b60c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "y_preds = []\n",
    "model.to(device)\n",
    "for x, y in valid_loader:\n",
    "    y_trues.append(y.cpu())\n",
    "    y_preds.append(model(x.to(device)).argmax(dim=1).cpu())\n",
    "\n",
    "y_true = torch.hstack(y_trues)\n",
    "y_pred = torch.hstack(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8a494-fd98-483a-b792-98ff40b05af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58043e-1b64-4f21-93e0-4c744bfbc652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
