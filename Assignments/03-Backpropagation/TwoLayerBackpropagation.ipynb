{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c7670f",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "![Two Layer Network Diagram](TwoLayerNetworkDiagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8744658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b0723",
   "metadata": {},
   "source": [
    "## Create fake input and output\n",
    "\n",
    "These are just randomly generated inputs and outputs for sake of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training examples\n",
    "N = 100\n",
    "\n",
    "# Number of inputs, hidden neurons, and outputs (based on diagram)\n",
    "nx = 3\n",
    "n1 = 3\n",
    "ny = 2\n",
    "\n",
    "# Each of the N examples has three (nx) real-valued inputs\n",
    "X = torch.randn(N, nx)\n",
    "\n",
    "# Each of the N examples has two (ny) independent binary classification outputs\n",
    "Y = torch.randint(low=0, high=2, size=(N, ny)).type(torch.float)\n",
    "\n",
    "# aka alpha or Î±\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5af1a0-83a8-45b1-9fe2-14e081c10e52",
   "metadata": {},
   "source": [
    "## Manual version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42456bc9",
   "metadata": {},
   "source": [
    "### Create a simple model based on the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(A, W, b):\n",
    "    return A @ W.T + b\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + torch.exp(-Z))\n",
    "\n",
    "\n",
    "class Manual2Layer:\n",
    "    \"A two-layer network.\"\n",
    "    \n",
    "    def __init__(self, nx, n1, ny, learning_rate):\n",
    "        # n0 and n2 are aliases for nx and ny\n",
    "        self.n0 = nx\n",
    "        self.n1 = n1\n",
    "        self.n2 = ny\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Layer 1 parameters\n",
    "        self.W1 = torch.randn(n1, nx)\n",
    "        self.b1 = torch.randn(n1)\n",
    "        \n",
    "        # Layer 2 parameters\n",
    "        self.W2 = torch.randn(ny, n1)\n",
    "        self.b2 = torch.randn(ny)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"Forward propagation.\"\n",
    "        # A0 is just an alias for the input, X\n",
    "        A0 = X\n",
    "    \n",
    "        # Forward propagation\n",
    "        Z1 = linear(A0, self.W1, self.b1)\n",
    "        self.A1 = sigmoid(Z1)\n",
    "        \n",
    "        Z2 = linear(self.A1, self.W2, self.b2)\n",
    "        self.A2 = sigmoid(Z2)\n",
    "        \n",
    "        # A2 is just an alias for the output, Yhat\n",
    "        Yhat = self.A2\n",
    "    \n",
    "        return Yhat\n",
    "\n",
    "    def bce_loss(self, Yhat, Y):\n",
    "        \"Compute loss as the binary cross-entropy loss.\"\n",
    "        self.Yhat = Yhat\n",
    "        self.Y = Y\n",
    "        return torch.mean(Y * torch.log(Yhat) + (1 - Y) * torch.log(1 - Yhat))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"Compute the gradients for all parameters.\"\n",
    "        # Compute gradients for W^[2] and b^[2]\n",
    "        dL_dY = (self.Y / self.Yhat - (1 - self.Y) / (1 - self.Yhat)) / 2\n",
    "        dY_dZ2 = self.Yhat * (1 - self.Yhat)\n",
    "        \n",
    "        dZ2 = dL_dY * dY_dZ2\n",
    "        \n",
    "        self.dW2 = (1 / N) * dZ2.T @ self.A1\n",
    "        self.db2 = dZ2.mean(dim=0)\n",
    "        \n",
    "        # Compute gradients for W^[1] and b^[1]\n",
    "        dZ1 = dZ2 @ self.W2 * ((self.A1 * (1 - self.A1)))\n",
    "        \n",
    "        self.dW1 = (1 / N) * dZ1.T @ X\n",
    "        self.db1 = dZ1.mean(dim=0)\n",
    "        \n",
    "    def update(self):\n",
    "        \"Update the parameter values.\"\n",
    "        self.W1 -= self.learning_rate * self.dW1\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "        self.W2 -= self.learning_rate * self.dW2\n",
    "        self.b2 -= self.learning_rate * self.db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7d473",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Manual2Layer(nx, n1, ny, learning_rate)\n",
    "\n",
    "# Compute the models initial output\n",
    "Yhat = model(X)\n",
    "bce_loss = model.bce_loss(Yhat, Y)\n",
    "print(f\"Manual loss before training: {bce_loss.item():0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a49c52",
   "metadata": {},
   "source": [
    "### Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients and update parameters\n",
    "model.backward()\n",
    "model.update()\n",
    "\n",
    "# Recompute loss and see if we've improved\n",
    "Yhat = model(X)\n",
    "bce_loss = model.bce_loss(Yhat, Y)\n",
    "print(f\"Manual loss after one step: {bce_loss.item():0.4f}\")\n",
    "print(\"The loss should be lower. (You can execute this cell multiple times.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa78642-6b65-4259-b4b2-b43fea1e479e",
   "metadata": {},
   "source": [
    "## Automatic version (using PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc82dbc",
   "metadata": {},
   "source": [
    "### Forward and backward propagation using PyTorch\n",
    "\n",
    "Copy the parameters from above, but configure them to use auto-differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48303e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Automatic2Layer:\n",
    "    \"This model copies parameters from the manual version for the sake of comparisons.\"\n",
    "    def __init__(self, manual_model):\n",
    "        self.learning_rate = manual_model.learning_rate\n",
    "        \n",
    "        # Layer 1 parameters\n",
    "        self.W1 = manual_model.W1.clone().detach().requires_grad_(True)\n",
    "        self.b1 = manual_model.b1.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Layer 2 parameters\n",
    "        self.W2 = manual_model.W2.clone().detach().requires_grad_(True)\n",
    "        self.b2 = manual_model.b2.clone().detach().requires_grad_(True)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"Forward propagation (same as above, but using PyTorch functionality).\"\n",
    "        A0 = X\n",
    "        Z1 = torch.nn.functional.linear(A0, self.W1, self.b1)\n",
    "        A1 = torch.sigmoid(Z1)\n",
    "        \n",
    "        Z2 = torch.nn.functional.linear(A1, self.W2, self.b2)\n",
    "        A2 = torch.sigmoid(Z2)\n",
    "        Yhat = A2\n",
    "\n",
    "        return Yhat\n",
    "\n",
    "    def bce_loss(self, Yhat, Y):\n",
    "        \"Compute loss as the binary cross-entropy loss.\"\n",
    "        self.loss = -torch.nn.functional.binary_cross_entropy(Yhat, Y)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"Compute the gradients for all parameters.\"\n",
    "        self.loss.backward()\n",
    "\n",
    "    def update(self):\n",
    "        \"Update the parameter values.\"\n",
    "        self.W1 -= self.learning_rate * self.W1.grad\n",
    "        self.b1 -= self.learning_rate * self.b1.grad\n",
    "        self.W2 -= self.learning_rate * self.W2.grad\n",
    "        self.b2 -= self.learning_rate * self.b2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efefe1d8-0290-4a4d-a664-5148180e883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new model and compute its output\n",
    "model2 = Automatic2Layer(model)\n",
    "Yhat = model2(X)\n",
    "\n",
    "model2.bce_loss(Yhat, Y)\n",
    "print(f\"Automatic loss before training: {bce_loss.item():0.4f}\")\n",
    "print(\"Compare this loss to the manually computed version above\")\n",
    "\n",
    "# Compute the gradients and update the parameters\n",
    "model2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2d38c",
   "metadata": {},
   "source": [
    "## Compare computed gradients\n",
    "\n",
    "We shouldn't compare floating-point numbers using \"==\" since results can differ based on the order of operations.\n",
    "\n",
    "You might get different results if you've run the cells above out-of-order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae248e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to compute new gradients for the manual model since we copied\n",
    "# the parameters AFTER the parameters were updated.\n",
    "model.backward()\n",
    "\n",
    "assert torch.allclose(model.dW2, model2.W2.grad)\n",
    "assert torch.allclose(model.db2, model2.b2.grad)\n",
    "\n",
    "assert torch.allclose(model.dW1, model2.W1.grad)\n",
    "assert torch.allclose(model.db1, model2.b1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2344b7-e489-47a5-b904-7aabcc62491d",
   "metadata": {},
   "source": [
    "## Suggestions\n",
    "\n",
    "Try\n",
    "\n",
    "- Adding additional layers\n",
    "- Changing the loss function\n",
    "- Changing the activation function(s)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
